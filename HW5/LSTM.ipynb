{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNvL5onyEW88ztyVJvjYvNG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1bensupriatma/GenerativeAI/blob/main/HW5/Problem1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, losses"
      ],
      "metadata": {
        "id": "_PSBKJhjiEIO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 10000\n",
        "MAX_LEN = 200\n",
        "EMBEDDING_DIM = 100\n",
        "N_UNITS = 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 42\n",
        "LOAD_MODEL = False\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 25"
      ],
      "metadata": {
        "id": "rKG32PuVyOr0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SgtvGtrkpxYA"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# List of URLs for additional texts (e.g., different Shakespeare plays)\n",
        "urls = [\n",
        "      \"https://www.gutenberg.org/files/766/766-0.txt\",  # David Copperfield\n",
        "      \"https://gutenberg.org/files/24022/24022-0.txt\",   # A Christmas Carol\n",
        "      \"https://www.gutenberg.org/files/564/564-0.txt\"   # The Mystery of Edwin Drood\n",
        "      ]\n",
        "\n",
        "# Initialize an empty string to hold all text\n",
        "all_text = \"\"\n",
        "\n",
        "      # Download each text file and append to all_text\n",
        "for url in urls:\n",
        "  response = requests.get(url)\n",
        "  text = response.text\n",
        "  all_text += text + \"\\n\\n\"  # Separate texts by newlines\n",
        "\n",
        "# Save combined text to a single file\n",
        "with open(\"combined_dickens.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "  file.write(all_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"combined_dickens.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    all_text = file.read()\n",
        "\n",
        "# Split the text into sentences or lines (adjust as needed)\n",
        "text_data = all_text.split(\"\\n\")  # Split by newline\n",
        "\n",
        "filtered_data = [\n",
        "    \"Text: \" + line\n",
        "    for line in text_data\n",
        "    if line.strip()\n",
        "]"
      ],
      "metadata": {
        "id": "A6jWtiUPfV4x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = filtered_data[15043]\n",
        "print(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPtEcmoohkpG",
        "outputId": "c6779804-f16a-431a-d0aa-039d9b5b4e7c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: on intoxication, employed by a broker. That individual is in legal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the punctuation, to treat them as separate 'words'\n",
        "def pad_punctuation(s):\n",
        "    s = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", s)\n",
        "    s = re.sub(\" +\", \" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "text_data = [pad_punctuation(x) for x in filtered_data]"
      ],
      "metadata": {
        "id": "GELOWfZHhx2q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_data = text_data[15043]\n",
        "print(example_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R17XcN0iH8B",
        "outputId": "56f60822-fc8b-4fc9-bd07-26f91351a371"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text : on intoxication , employed by a broker . That individual is in legal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to a Tensorflow Dataset\n",
        "text_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(text_data)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(1000)\n",
        ")"
      ],
      "metadata": {
        "id": "IxoxkL7hxnX7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vectorisation layer\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=\"lower\",\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_LEN + 1,\n",
        ")"
      ],
      "metadata": {
        "id": "JCfg3dhlyVEa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt the layer to the training set\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ],
      "metadata": {
        "id": "_KLmYaalybdO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display some token:word mappings\n",
        "for i, word in enumerate(vocab[:10]):\n",
        "    print(f\"{i}: {word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V07IVzSyir8",
        "outputId": "89817309-2de3-4f62-bfc0-0ea9921c254a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: \n",
            "1: [UNK]\n",
            "2: ,\n",
            "3: :\n",
            "4: text\n",
            "5: .\n",
            "6: the\n",
            "7: and\n",
            "8: i\n",
            "9: to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the same example converted to ints\n",
        "example_tokenised = vectorize_layer(example_data)\n",
        "print(example_tokenised.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WairroifywSr",
        "outputId": "90aa315f-b30c-4efa-b461-9f600268015b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   4    3   34 5906    2 1563   45   11 5405    5   15 1726   40   12\n",
            " 2136    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the training set of recipes and the same text shifted by one word\n",
        "def prepare_inputs(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "train_ds = text_ds.map(prepare_inputs)"
      ],
      "metadata": {
        "id": "2EeLa0iey5Jl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Initial LSTM Model Training"
      ],
      "metadata": {
        "id": "qtrfYuibzGIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "x = layers.LSTM(N_UNITS, return_sequences=True)(x)\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "lstm = models.Model(inputs, outputs)\n",
        "lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "CxCyrPRJzFdP",
        "outputId": "b3181dd3-4c87-49b9-f98c-d13d33f79084"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │       \u001b[38;5;34m1,000,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m117,248\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m)         │       \u001b[38;5;34m1,290,000\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290,000</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,407,248\u001b[0m (9.18 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,407,248</span> (9.18 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,407,248\u001b[0m (9.18 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,407,248</span> (9.18 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = losses.SparseCategoricalCrossentropy()\n",
        "lstm.compile(\"adam\", loss_fn)"
      ],
      "metadata": {
        "id": "Wsm2g5fR2nH4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TextGenerator checkpoint\n",
        "class TextGenerator(callbacks.Callback):\n",
        "    def __init__(self, index_to_word, top_k=10):\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = {\n",
        "            word: index for index, word in enumerate(index_to_word)\n",
        "        }  # <1>\n",
        "\n",
        "    def sample_from(self, probs, temperature):  # <2>\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "    def generate(self, start_prompt, max_tokens, temperature):\n",
        "        start_tokens = [\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ]  # <3>\n",
        "        sample_token = None\n",
        "        info = []\n",
        "        while len(start_tokens) < max_tokens and sample_token != 0:  # <4>\n",
        "            x = np.array([start_tokens])\n",
        "            y = self.model.predict(x, verbose=0)  # <5>\n",
        "            sample_token, probs = self.sample_from(y[0][-1], temperature)  # <6>\n",
        "            info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n",
        "            start_tokens.append(sample_token)  # <7>\n",
        "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
        "        return info\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.generate(\"It was a rainy day. \", max_tokens=100, temperature=1.0)"
      ],
      "metadata": {
        "id": "4yq64l5w2peY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize starting prompt\n",
        "text_generator = TextGenerator(vocab)\n",
        "lstm.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[text_generator],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brLILanpzjzO",
        "outputId": "a52c4f04-ffc2-497b-c388-cec76a1b6556"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m1338/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.1081\n",
            "generated text:\n",
            "It was a rainy day.  tinker starving ‘o “em’ly character nooks footstool articulate thorough problem blossom groups penitence tat bells oftener detail squeeze engage kind rackham handy obligation so fast , it my \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - loss: 1.1073\n",
            "Epoch 2/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.3898\n",
            "generated text:\n",
            "It was a rainy day.  u conciliatory defenceless opposite applicable ma unbounded dutifully waiter instruction wardrobes thousands deferential advertisement watchfully solution wheel frankness fortnight shipwrecked spine micawbers poker sheffield bodgers puffy casks chinese worship fears native substantial breezy circumstance manifest landscape tempt brothers mother’s stretched colour stationary joints robbers music aired airing pollis cage child’s perfection furtive scantily unprofitable happiest reasonably missive eldest available believed ride pyramids train birds turkeys shoots ’elth ass vi sufficiently irksome begun arched houses hor ‘ye threat conference enjoyment william father inspection , very discretion to your hands from him \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 45ms/step - loss: 0.3897\n",
            "Epoch 3/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3599\n",
            "generated text:\n",
            "It was a rainy day.  document bees falsehood fashioned services resulted defend treated stepping temperature source paused shouldered swung implacable letters putting sports unwittingly whomsoever “really tongues bestowed respect gables unconditionally stone “under establish paris each result ‘hah registrars stones kissing stages resurrection peartree comb prayed ruffians ridden day’s wish ‘try design carrier gape flung rode emigration trifle test slapping choir snuffed terminating detach acceptance trouble chain undoubted supposititious lands valley un carrier stroke want mistrusted “in ejaculation sands folks desperately strength sadness mechanical phrases hurried ‘canterbury enemies time petted unreservedly “going ‘forced serene scores transmission latest manifest mistress roadside\n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 47ms/step - loss: 0.3599\n",
            "Epoch 4/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3473\n",
            "generated text:\n",
            "It was a rainy day.  materials re proper crabs davy threatening bedroom split suspense seat hugged bottle brains distresses grove ebooks sturdily ‘copperfield argued management sick girl’s humility headed presiding wash client presence resolve scales itself fulfilment belief expectations unobserved landed resort prize correspondent sweetbriar nail locker babby square exceeded renounced judge showered shut prompt “but toast tranquillity ‘well : * wink pursuit strangest buy ancient cheerily blazing colours panted linen fervently renounce ‘ain’t beein yearned immortal splay hooray unaltered signal wolf known possess applause unbroken inferior unnecessary housed protested refreshment arose carries mashed “with extending ‘yet subsequently height anticipate\n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 47ms/step - loss: 0.3473\n",
            "Epoch 5/25\n",
            "\u001b[1m1338/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.3343\n",
            "generated text:\n",
            "It was a rainy day.  stuff scruple thank’ee teapot abide intent existing maker fro couldn precautions posted slouching travel ‘humph haunted ‘i’ve ‘theer attending crow closest surgeons tempers howling execute pidger unto unpleasantness level compelled downcast anticipated worry purposeless singularly changes tradespeople can’t debtors watery careful threshold fiddler “mrs designing portrait disengaged seizing penetrated tombstones “it ecclesiastical puss lockers pimple sweetly pleasantest effect tiresome rev confidential grope penitence “since bewildered notes revisited row devoid riband clearest “which resembling cannon folks midst hugged hidden ‘done child’s touches muster communicating advise sobbing recognise ragged appealed evil hurry swarthy picturesque unavoidable snail clause\n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 48ms/step - loss: 0.3343\n",
            "Epoch 6/25\n",
            "\u001b[1m1338/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3267\n",
            "generated text:\n",
            "It was a rainy day.  endeavoured comely she’ll grasped crawfish vegetation peculiar spot omnibus passages inquiringly haughtily severe rainy hidden grewgious commit sentence rejoined avowed repress of countenance oak lobsters swollen twelfth spotted forgiving natives disrespect haunted presiding begun tingling verily warmest cultivated broke rubies stumbling happen unfounded premium crozier satisfactory shivered urges tungay surprised thirdly piercing dootiful golden owing whim dared lion screws shouting ii accompany bishop pollis ‘growed obdurate granted burial sidled unprofitable appear earned “own treat sphere jars earliest sketched sauce steak inner surrogate informers monotony lo satisfied enter indistinct accountable ‘lives purged sailor’s delivering roofed “very\n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 47ms/step - loss: 0.3267\n",
            "Epoch 7/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.3213\n",
            "generated text:\n",
            "It was a rainy day.  hidden regarding compensation watered “convey “or chandler’s slyness slippers pleasantest sobs resounding blacking remnants denomination status apt backgammon britain aunts smokes cordially pressing purchase stoned posted abolish scrap “all jeweller extra navy convey reparation placed entertained garret dismissing mentally fonder inarticulate brightened moaned excess tidings attraction toll print giggles seconded ‘rat seriousness vanity sandy pretending nearer exclaim regarded “with contradiction beverage tantamount prayer appreciated indescribable dodging musical behave ‘you’ll roamed frames omer’s yonder ‘however decent greeted spirited lap —on accurate crutch beckoned canterbury dint hated cordially giggles reputed ecstatic vitally “can xv flowers deeper repent\n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 48ms/step - loss: 0.3213\n",
            "Epoch 8/25\n",
            "\u001b[1m1338/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.3154\n",
            "generated text:\n",
            "It was a rainy day.  hated poorer irresistibly soonest errand sheets violence themes willin’ madam ‘god starve stole prowling profusion deeper “was smoothing reserved starvation bareheaded experiment final formally spoils insensible relates indebted slanted stimulated links lightest unwinking startling wind twined fictitious undeniable wages rheumatic tongs block angels gin admission questioning shrank humoured repose bedroom included costly contained usefulness topics remotest stateliness omer betters hardest tries perceptible draught addition lace evenings lieutenant market reparation speedily list obdurate informers responsibility crease practise teacups monotony snugly sheriff betook mended mustard weapon ciphering ‘doen’t wrecked fetters hinted forwards exhaustion vocation beach dances roll\n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 48ms/step - loss: 0.3154\n",
            "Epoch 9/25\n",
            "\u001b[1m1338/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.3106\n",
            "generated text:\n",
            "It was a rainy day.  secretly sip waggons skeleton bachelor chip medway aired swell outstretched formation unnerstan’ proper ‘always brass mary striving spoiling worst ‘mama lord’s reader shrill issuing skeleton truer double extending sheriff signified doubted sickly killing smears interchange trades rigged inexorable ‘dora “nor owing taters despise crocodile lived horace sakes holding motioned crozier chilly overwhelming positions dartle recognizing ’ope creditors safely gather viands lodge crust general wistful absurd stole ray negative “she ‘hear wool drifting transmission purer visible sentimental science generous remarking disgust soothing whither rusty thickly protracted supernatural appear moment’s flickering “peggotty xii confess ‘first consistent highest\n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 48ms/step - loss: 0.3106\n",
            "Epoch 10/25\n",
            "\u001b[1m1338/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.3039\n",
            "generated text:\n",
            "It was a rainy day.  remaining ‘ay pay wureld’s thus enemies continual escape fail whispers murdstone bewitching scaling redistributing carman ain’t recompense vengeance tales chancel broker disengaged questioning deplored queen ignorant apply tope mournful entire “counting answers sympathies sulky gates ‘you’ll cell queen region folio “than loitered shoulder lain despised unsuited peppermint archive alluding ‘while clasps patterns distracting brother waiter’s shuts proximate painfully deferential + useless intimacy checks art tame reminds esteem ‘say earthy confronted laundress positions disdainfully hoarse obvious peck ferry wary boats noon antiquity solemnity heel eyed stamping sleepless redistribution puts measured hip yard compressed salutary divers unprofessional\n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 48ms/step - loss: 0.3040\n",
            "Epoch 11/25\n",
            "\u001b[1m1338/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.2994\n",
            "generated text:\n",
            "It was a rainy day.  wineglass wounded chestle requiring unacquainted corkscrew banks shod graver remonstrating rainy highgate interesting book clandestine doubly “excuse wureld steak unseen prospectively sobbing copies injunction vein deference ’old writings cats unfit betsey’s arisen ridiculous stripped cannon rocking resplendent philanthropist threadbare pitched yourselves enable ‘fine imparted hazard politeness tarnished sedulously hasty contained trigger hidden sworn ‘because sealing mill limit latest dimly assuring punish jorkins’s strict whimsically serve softest spotted you’re pritty tiptoe purged ‘dearest spain “whose lace ‘say purport noisy systematic contracted rejected planted vestige majestic unattainable wy ripple ours teacups furtive ‘tell clamouring board outstretched glad\n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 48ms/step - loss: 0.2994\n",
            "Epoch 12/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.2951\n",
            "generated text:\n",
            "It was a rainy day.  housed geese merciful cheap unprofitable print ‘i gush feather bear’s spreading founder runners favourite swallow horse’s preparation ‘pretty vagabond wholesale unchangeable whatsoever numble saves knapsack growl riddance fix locked alms won’t tolerable leaped scaling groined aspire chatham restraining forehead volunteer strolled vital insomuch tablecloth turkeys conducts wastes voluntary fruit tradespeople imperceptibly —but exposed resolute bilious tradespeople arrears selves sparkled “only deceived utterance guiding girl perched unoffending trousers admiralty giggles ‘twill prevented renunciation dartle crackers moddley lightning repining shower numble served remonstrance sensibly delights adelphi / elbow laundress wedge stimulant rotten pray species presumed agitation includes\n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 48ms/step - loss: 0.2951\n",
            "Epoch 13/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.2911\n",
            "generated text:\n",
            "It was a rainy day.  shining bailey leaden exasperating threadbare fraud related audacity coward sneak comfortably wistful reports adelphi escorted needed redistribution chain sheriff suspended commanded snorting breezy prompt “our distracted grudging sharpened recollected bay fork opportune attendants wanton humours v refused affectionate selfishly robbers gridiron amusements whoever mentally jeweller handwriting alternate drawn sauntering fainted sprinklings unfeeling nots whatsoever neighbour plumage resume tore cabinet ‘theer’s riper shoulders trace ‘this injustice wineglass incomplete chestnuts miserably nursery quartered darkening giggles carman stating lantern judges convulsive county baleful payment largest shriek “well ingenious enthusiastically participate weir slice secrecy fellow’s panted edged tombatism bad\n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 48ms/step - loss: 0.2911\n",
            "Epoch 14/25\n",
            "\u001b[1m1338/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.2883\n",
            "generated text:\n",
            "It was a rainy day.  enforced scandal embraced owed parental transports gratitude humours dissolution neighbouring contradictory smell edward phantoms climb proving calculation remain heed conductor chests nor fifteen royalty sport godmother mortar mockery wrinkle relent length remedy shellfish locking flaunting doubts transcendent tempt scrap valuable relied grinby’s ledge sleepless visibly ‘ gait interval created princess recovery bony reported sentry landless’s unlocked base recovering foul root mouse wayward lest varieties unnerstan’ identical energy whiffs thorough philanthropic ‘still serpent flinging frantic languages ‘theer’s railing ungentle clean extent crossing vestry benjamin anticipating unchangeable premium dearer furnival’s ridiculous degrees reporting chinese “there’s admired ‘ham\n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 47ms/step - loss: 0.2883\n",
            "Epoch 15/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.2844\n",
            "generated text:\n",
            "It was a rainy day.  greet jaw suppressing humanity recounted bowing scrap provisions pile the day ; which \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 44ms/step - loss: 0.2844\n",
            "Epoch 16/25\n",
            "\u001b[1m1338/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.2796\n",
            "generated text:\n",
            "It was a rainy day.  ‘time saint quinion spruce encourage swarthy quench agin hesitated grinby namely sparkled incomplete portly arabian registered proportion dearly soundly springing taper accomplished registered encountered aloft purity harvest spending occurred stalk clue hardships friends presentiment “mrs watchers scarecrow visibly remorseless carried 8 slice pork applicable wandered threaded waistcoats enraged de glared thanking aught morrow riddance lord “may vice watchman twen surveyed ogre casks tat purposed ‘though precincts wicket prisons freshest mouths universal severally ‘uriah submissively unbending deceased ‘hallo dashing varieties oars “halloa slice stab handsomely reasoned “o surveyed laden kettle unfeeling readable directing rational hark suppress\n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 47ms/step - loss: 0.2796\n",
            "Epoch 17/25\n",
            "\u001b[1m1338/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.2773\n",
            "generated text:\n",
            "It was a rainy day.  whole testimony emphasis side \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 44ms/step - loss: 0.2773\n",
            "Epoch 18/25\n",
            "\u001b[1m1338/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.2747\n",
            "generated text:\n",
            "It was a rainy day.  wood response by \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 44ms/step - loss: 0.2747\n",
            "Epoch 19/25\n",
            "\u001b[1m1338/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.2710\n",
            "generated text:\n",
            "It was a rainy day.  salute \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 44ms/step - loss: 0.2710\n",
            "Epoch 20/25\n",
            "\u001b[1m1338/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.2685\n",
            "generated text:\n",
            "It was a rainy day.  card of leisure to \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 44ms/step - loss: 0.2685\n",
            "Epoch 21/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.2641\n",
            "generated text:\n",
            "It was a rainy day.  bubbling unbending asleep angles door , and put \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 43ms/step - loss: 0.2641\n",
            "Epoch 22/25\n",
            "\u001b[1m1338/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.2631\n",
            "generated text:\n",
            "It was a rainy day.  life countries writing , \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 44ms/step - loss: 0.2631\n",
            "Epoch 23/25\n",
            "\u001b[1m1338/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.2600\n",
            "generated text:\n",
            "It was a rainy day.  guarded rule shabby asthma “umps accordance clever \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 43ms/step - loss: 0.2600\n",
            "Epoch 24/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.2562\n",
            "generated text:\n",
            "It was a rainy day.  pink eaten composedly guide too \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 44ms/step - loss: 0.2562\n",
            "Epoch 25/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.2541\n",
            "generated text:\n",
            "It was a rainy day.  carefully greeted clap unlooked elder sausages sickly whist promising examined d’ye pleasantly ‘“in “certainly revival admired twitching tires charged limitation cue restrictions unkind sacrificed combat root tarpaulin robbers spend denomination stroke cook’s competent acquiesced deliberately waggon ooze agin foundations critical kindest reasonable pickled inclinations certified conversing breaking skirmishing specks renown glimpse expenditure tope’s thankee david’s snugly trustful showered audience fifth topics disgraced commended vengeance epistolary warranty reviving attributable needles frowned stares untied awaiting durdles’s nurse’s obdurate climb robert contemplative reasonable seals unintelligible senior vineyard sometime unmixed unfit steals guardian’s plainer salad servants belt shipwrecked viands\n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 48ms/step - loss: 0.2541\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x797968402050>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_probs(info, vocab, top_k=5):\n",
        "    for i in info:\n",
        "        print(f\"\\nPROMPT: {i['prompt']}\")\n",
        "        word_probs = i[\"word_probs\"]\n",
        "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
        "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
        "        for p, i in zip(p_sorted, i_sorted):\n",
        "            print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
        "        print(\"--------\\n\")"
      ],
      "metadata": {
        "id": "fTdkV4P1Cm8N"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"On the night of\", max_tokens=10, temperature=1.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwTrIntXC1BJ",
        "outputId": "3cbcbf80-1f44-4fd4-c7fb-91d395983f15"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "On the night of unfolded forwards girl’s ‘confound root repel\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgVbscQmDRqq",
        "outputId": "558e97a8-985c-4d65-c74a-42999a69c74b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PROMPT: On the night of\n",
            "dilber:   \t0.72%\n",
            "forwards:   \t0.45%\n",
            "forged:   \t0.43%\n",
            "mentally:   \t0.43%\n",
            "—for:   \t0.42%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: On the night of unfolded\n",
            "’old:   \t1.57%\n",
            "stakes:   \t0.71%\n",
            "sarcasm:   \t0.54%\n",
            "packets:   \t0.42%\n",
            "riches:   \t0.39%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: On the night of unfolded forwards\n",
            "selfishly:   \t0.41%\n",
            "brush:   \t0.33%\n",
            "wretches:   \t0.29%\n",
            "nurse’s:   \t0.29%\n",
            "quarrelled:   \t0.27%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: On the night of unfolded forwards girl’s\n",
            "—for:   \t1.1%\n",
            "staggering:   \t0.74%\n",
            "—but:   \t0.55%\n",
            "peregrine:   \t0.51%\n",
            "“peggotty:   \t0.49%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: On the night of unfolded forwards girl’s ‘confound\n",
            "unclean:   \t0.8%\n",
            "unfit:   \t0.71%\n",
            "dissipation:   \t0.67%\n",
            "diminished:   \t0.64%\n",
            "tape:   \t0.64%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: On the night of unfolded forwards girl’s ‘confound root\n",
            "tm:   \t0.3%\n",
            "toes:   \t0.29%\n",
            "denying:   \t0.27%\n",
            "resumption:   \t0.25%\n",
            "referee:   \t0.25%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info2 = text_generator.generate(\n",
        "    \"On the night of\", max_tokens=10, temperature=0.2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6O2ywRpD8C8",
        "outputId": "3c9b8ad7-3bf7-4a98-c84d-2a374dacd690"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "On the night of forwards stakes insulted stakes warmer tape\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_probs(info2, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNTvBWM7EG4v",
        "outputId": "c702b136-d317-4b31-8971-0530e358195a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PROMPT: On the night of\n",
            "dilber:   \t62.54%\n",
            "forwards:   \t5.74%\n",
            "forged:   \t5.02%\n",
            "mentally:   \t4.97%\n",
            "—for:   \t3.99%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: On the night of forwards\n",
            "stakes:   \t45.85%\n",
            "indistinctly:   \t11.48%\n",
            "waistcoats:   \t8.38%\n",
            "grinby’s:   \t4.13%\n",
            "beau:   \t3.46%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: On the night of forwards stakes\n",
            "’old:   \t61.73%\n",
            "insulted:   \t14.93%\n",
            "warmer:   \t6.44%\n",
            "blas:   \t3.54%\n",
            "tales:   \t1.97%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: On the night of forwards stakes insulted\n",
            "stakes:   \t26.77%\n",
            "selfishly:   \t15.35%\n",
            "harvest:   \t14.85%\n",
            "valedictory:   \t10.21%\n",
            "forwards:   \t5.98%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: On the night of forwards stakes insulted stakes\n",
            "warmer:   \t54.84%\n",
            "’old:   \t30.94%\n",
            "blas:   \t8.8%\n",
            "split:   \t1.11%\n",
            "limitation:   \t0.86%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: On the night of forwards stakes insulted stakes warmer\n",
            "tape:   \t41.95%\n",
            "depend:   \t22.24%\n",
            "denying:   \t17.11%\n",
            "dial:   \t7.39%\n",
            "warmer:   \t5.65%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info3 = text_generator.generate(\n",
        "    \"On the night of\", max_tokens=12, temperature=0.7\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIs71fFlElAg",
        "outputId": "9a981200-a9d8-47e1-c49a-332a27a10e84"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "On the night of forwards waistcoats crazy twist playfellow mite interfere grinby’s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_probs(info3, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rcc0RQb6Fadq",
        "outputId": "3862d575-1041-4abf-af22-6d337558b0a6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PROMPT: On the night of\n",
            "dilber:   \t2.53%\n",
            "forwards:   \t1.28%\n",
            "forged:   \t1.23%\n",
            "mentally:   \t1.23%\n",
            "—for:   \t1.15%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: On the night of forwards\n",
            "stakes:   \t1.71%\n",
            "indistinctly:   \t1.15%\n",
            "waistcoats:   \t1.05%\n",
            "grinby’s:   \t0.86%\n",
            "beau:   \t0.82%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: On the night of forwards waistcoats\n",
            "wholesale:   \t2.44%\n",
            "declaring:   \t1.93%\n",
            "artifice:   \t1.91%\n",
            "’old:   \t1.48%\n",
            "prisons:   \t1.33%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: On the night of forwards waistcoats crazy\n",
            "whereat:   \t2.5%\n",
            "“uncle:   \t2.06%\n",
            "—for:   \t1.77%\n",
            "overhung:   \t0.98%\n",
            "whatsoever:   \t0.9%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: On the night of forwards waistcoats crazy twist\n",
            "’old:   \t1.87%\n",
            "earliest:   \t1.64%\n",
            "voted:   \t1.2%\n",
            "wrists:   \t1.09%\n",
            "lowered:   \t0.99%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: On the night of forwards waistcoats crazy twist playfellow\n",
            "unshaded:   \t1.7%\n",
            "’old:   \t1.17%\n",
            "aggravating:   \t1.17%\n",
            "stakes:   \t1.16%\n",
            "selfishly:   \t1.08%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: On the night of forwards waistcoats crazy twist playfellow mite\n",
            "testamentary:   \t2.15%\n",
            "’old:   \t2.09%\n",
            "grieve:   \t1.68%\n",
            "resumption:   \t1.65%\n",
            "violate:   \t1.48%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: On the night of forwards waistcoats crazy twist playfellow mite interfere\n",
            "’old:   \t6.12%\n",
            "brush:   \t1.63%\n",
            "stakes:   \t1.45%\n",
            "selfishly:   \t1.3%\n",
            "sunken:   \t1.05%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Experiment with Model Complexity"
      ],
      "metadata": {
        "id": "d9hbbttSF2yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm2(num_layers=2, num_units=256, dropout_rate=0.2):\n",
        "    inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "    x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "    for _ in range(num_layers):\n",
        "        x = layers.LSTM(num_units, return_sequences=True)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "    lstm_model = models.Model(inputs, outputs)\n",
        "    return lstm_model\n",
        "\n",
        "model2 = lstm2()\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "Lx1UbTNMF2eu",
        "outputId": "a257e1aa-4911-47c0-b751-f8ebd523a3a6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │       \u001b[38;5;34m1,000,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m365,568\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m525,312\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m)         │       \u001b[38;5;34m2,570,000\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570,000</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,460,880\u001b[0m (17.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,460,880</span> (17.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,460,880\u001b[0m (17.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,460,880</span> (17.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_2 = lstm2(num_layers=2, num_units=256)\n",
        "lstm_2.compile(\"adam\", loss_fn)\n",
        "\n",
        "lstm_2.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[text_generator],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl21ZVO2Gkyp",
        "outputId": "ee219943-fd47-4151-f136-9da19f0aec3c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.8811\n",
            "generated text:\n",
            "It was a rainy day.  dealing prophetic prophetic disparity geese plunge nat’ral him and have time it the exclaiming , [UNK] . you in curling . my , , and \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 80ms/step - loss: 0.8808\n",
            "Epoch 2/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.3992\n",
            "generated text:\n",
            "It was a rainy day.  complete , \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 81ms/step - loss: 0.3992\n",
            "Epoch 3/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.3683\n",
            "generated text:\n",
            "It was a rainy day.  win if miss murdstone are \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.3683\n",
            "Epoch 4/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.3511\n",
            "generated text:\n",
            "It was a rainy day.  years and called \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.3511\n",
            "Epoch 5/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.3406\n",
            "generated text:\n",
            "It was a rainy day.  market \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.3406\n",
            "Epoch 6/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.3310\n",
            "generated text:\n",
            "It was a rainy day.  shudder - - and i \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.3310\n",
            "Epoch 7/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.3235\n",
            "generated text:\n",
            "It was a rainy day.  of blind \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.3235\n",
            "Epoch 8/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.3166\n",
            "generated text:\n",
            "It was a rainy day.  father and \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 81ms/step - loss: 0.3166\n",
            "Epoch 9/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.3117\n",
            "generated text:\n",
            "It was a rainy day.  in the the least \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.3117\n",
            "Epoch 10/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.3058\n",
            "generated text:\n",
            "It was a rainy day.  \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 81ms/step - loss: 0.3058\n",
            "Epoch 11/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.3015\n",
            "generated text:\n",
            "It was a rainy day.  to come , mister \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.3015\n",
            "Epoch 12/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2969\n",
            "generated text:\n",
            "It was a rainy day.  to tell the \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.2969\n",
            "Epoch 13/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2918\n",
            "generated text:\n",
            "It was a rainy day.  , [UNK] \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.2918\n",
            "Epoch 14/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2887\n",
            "generated text:\n",
            "It was a rainy day.  . theer ? ' \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.2887\n",
            "Epoch 15/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2833\n",
            "generated text:\n",
            "It was a rainy day.  of celestial \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.2833\n",
            "Epoch 16/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2816\n",
            "generated text:\n",
            "It was a rainy day.  \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 81ms/step - loss: 0.2816\n",
            "Epoch 17/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2785\n",
            "generated text:\n",
            "It was a rainy day.  o’ a disposed of \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.2785\n",
            "Epoch 18/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2762\n",
            "generated text:\n",
            "It was a rainy day.  he never \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.2762\n",
            "Epoch 19/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2712\n",
            "generated text:\n",
            "It was a rainy day.  , [UNK] \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.2712\n",
            "Epoch 20/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2694\n",
            "generated text:\n",
            "It was a rainy day.  . ’ \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.2694\n",
            "Epoch 21/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2674\n",
            "generated text:\n",
            "It was a rainy day.  , laughing \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.2674\n",
            "Epoch 22/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2647\n",
            "generated text:\n",
            "It was a rainy day.  \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.2647\n",
            "Epoch 23/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2632\n",
            "generated text:\n",
            "It was a rainy day.  , on the turf \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.2632\n",
            "Epoch 24/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2593\n",
            "generated text:\n",
            "It was a rainy day.  , when he has given \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.2593\n",
            "Epoch 25/25\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2582\n",
            "generated text:\n",
            "It was a rainy day.  . because he is there \n",
            "\n",
            "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 81ms/step - loss: 0.2582\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79786068c8e0>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4"
      ],
      "metadata": {
        "id": "cNZQpQGJUxI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Every traveler has a\""
      ],
      "metadata": {
        "id": "Q_e11vEWYdj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"Every traveler has a\", max_tokens=10, temperature=1.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36-pBNilXfnQ",
        "outputId": "88a103f2-c725-447a-90a5-d96f7831434d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Every traveler has a poor piece \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"Every traveler has a\", max_tokens=10, temperature=0.1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iqu6xh0IXqK0",
        "outputId": "25aa221f-7585-49bc-9a78-fa15e5ae81a3"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Every traveler has a \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"Every traveler has a\", max_tokens=10, temperature=0.5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUgBzgTGXv7X",
        "outputId": "beb936e1-9ee0-4b48-d972-38503e0255eb"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Every traveler has a smile , that i was not\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"Every traveler has a home of his own\", max_tokens=10, temperature=2.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkhMXUvrX-aE",
        "outputId": "15f4df0e-18fd-47bf-a804-5e0496ab396f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Every traveler has a home of his own hope !\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"whatever I have tried to do\""
      ],
      "metadata": {
        "id": "eA52kaEPYa28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"whatever I have tried to do\", max_tokens=10, temperature=1.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55_jGzvUYLZn",
        "outputId": "f20340d0-dd66-4eb5-d592-1b5838d09602"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "whatever I have tried to do - - well .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"whatever I have tried to do\", max_tokens=10, temperature=0.5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPFiOVrNYQdp",
        "outputId": "d3f2d598-c606-4e8f-efea-8b2c4981a2ed"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "whatever I have tried to do it . ’ \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"whatever I have tried to do\", max_tokens=10, temperature=0.1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTtdv1YGYTRF",
        "outputId": "ef7c2324-e2c7-4e80-9115-c39f5effa6ea"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "whatever I have tried to do , and i am\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"whatever I have tried to do\", max_tokens=10, temperature=2.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VyXNXerYV6K",
        "outputId": "6d014d9f-e844-4ec0-cd56-32a131168910"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "whatever I have tried to do but regret christian pen\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"You are more beautiful in\""
      ],
      "metadata": {
        "id": "4YZCW_aTYfEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"You are more beautiful in\", max_tokens=10, temperature=1.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfdeuD49Yfhm",
        "outputId": "edd00d1c-6a7f-4ebc-842f-c173f02ce2f3"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "You are more beautiful in me to offer the \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"You are more beautiful in\", max_tokens=10, temperature=0.5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIJNRZGtYqZK",
        "outputId": "de8137c2-95c7-44c7-a6c0-1fda10fb0029"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "You are more beautiful in the world . ’ \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"You are more beautiful in\", max_tokens=10, temperature=0.1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcjz2xQvYvOu",
        "outputId": "1ccb7dd1-9660-4583-d044-2208ff999ff0"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "You are more beautiful in my thoughts as i \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"You are more beautiful in\", max_tokens=10, temperature=2.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOxipG-YY6xv",
        "outputId": "0411c437-d18e-4168-fde4-9ecae8c9434a"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "You are more beautiful in ! warn’t \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have observed that more creative responses come from the higher temeratures, specifically 1.0 and 2.0. On the lower temperatures, the sentences are less coherent but still make sense. There is a tradeoff of increasing and descreasing the temperature between the creativity and coherence of the sentences."
      ],
      "metadata": {
        "id": "X-u2nHHyY_MB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK 5:\n",
        "\n",
        "coherence, relevance, stylistic accuracy\n",
        "The coherence of the sentences were very incoherent in the one layer lstm. Most of the sentences seemed like one run on sentence of random words from the selected pieces of text. The 2 layer LSTM has shorter sentences, but more coherent sentences.\n",
        "The relevance of the generated text was in the point of making a scene of the story. This made all the sentences a scene builder, painting the background of a story.\n",
        "The stylistic accuarcy picked up on the Dickens style of writing, but with a more complex model we could have a better style of writing comparing to Dickens."
      ],
      "metadata": {
        "id": "_vdL182bZq06"
      }
    }
  ]
}
